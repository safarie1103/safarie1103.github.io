{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue> Real Time Flight Data Streaming - Kafka Producer </font>\n",
    "**Author:**  Armin Berger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing the streaming data, where you can use csv modules to read and publish the data to the Kafka stream.\n",
    "\n",
    "### Flight Data Overview:\n",
    "The flight-delays and cancellation data was collected and published by the U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics. This data records the flights operated by large air carriers and tracks the on-time performance of domestic flights. This data summarises various flight information such as the number of on-time, delayed, cancelled, and diverted flights published in DOT's monthly in 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import csv\n",
    "from datetime import timezone\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('D:/GitHub/dsc650/ExternalResources/stock-market-technical-indicators-visualization/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mchdir(data_dir\u001b[38;5;241m.\u001b[39mjoinpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStocks\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir()\n\u001b[0;32m      3\u001b[0m number_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir.joinpath('Stocks'))\n",
    "list = os.listdir()\n",
    "number_files = len(list)\n",
    "print(number_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_dir.joinpath('Stocks'))\n",
    "list = os.listdir()\n",
    "number_files = len(list)\n",
    "print(number_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "#### Read in clean csv data for streaming\n",
    "\n",
    "Write a python program that loads all the data from “flight*.csv”. Save the file as Task1_flight_producer.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in the csv flight files\n",
    "def readCSVFile():\n",
    "\n",
    "    # empty list for all dfs \n",
    "    flight_dict_list = []\n",
    "\n",
    "    # load all of the csv files as dfs, 20 flight csv files\n",
    "    for i in range(1,21):\n",
    "        \n",
    "        # open each flight csv file\n",
    "        with open(flights_dir.joinpath(f'flight{i}.csv'), 'rt') as f:\n",
    "            \n",
    "            # read in the csv as a dict \n",
    "            reader = csv.DictReader(f)\n",
    "            \n",
    "            # loop through every single dictionary\n",
    "            for row in reader:\n",
    "                \n",
    "                # append each dict to the list \n",
    "                flight_dict_list.append(row)\n",
    "    \n",
    "    # return a list of dictionaries\n",
    "    return flight_dict_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "#### Prepare data for day_of_week usage \n",
    "\n",
    "The keyFlights are generated from the column ‘DAY_OF_WEEK’ in the dataset which has 7 unique keys. These values 1, 2, 3, 4, 5, 6, and, 7 represents ‘sunday’,‘monday’, ‘tuesday’, ‘wednesday’,’thursday’,’friday’,’saturday’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that saves each observation in a dict of lists according to her \n",
    "def prepare_data(all_data):\n",
    "    \n",
    "    # dict of lists containing flight data for each of the 7 unique keys\n",
    "    dow_dict = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[]}\n",
    "\n",
    "    # loop through all days of the week\n",
    "    for i in range(0,len(all_data)):\n",
    "\n",
    "        if all_data[i]['DAY_OF_WEEK'] == '1':\n",
    "\n",
    "            dow_dict[1].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '2':\n",
    "\n",
    "            dow_dict[2].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '3':\n",
    "\n",
    "            dow_dict[3].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '4':\n",
    "\n",
    "            dow_dict[4].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '5':\n",
    "\n",
    "            dow_dict[5].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '6':\n",
    "\n",
    "            dow_dict[6].append(all_data[i])\n",
    "\n",
    "        elif all_data[i]['DAY_OF_WEEK'] == '7':\n",
    "\n",
    "            dow_dict[7].append(all_data[i])\n",
    "    \n",
    "    return dow_dict\n",
    "\n",
    "# function that saves the lenght of a list and saves it as a key value pair\n",
    "def get_len_data(data):\n",
    "    \n",
    "    len_data = {}\n",
    "\n",
    "    for key,value in data.items():\n",
    "\n",
    "        len_data[key] = len(value)\n",
    "    \n",
    "    return len_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "#### Create publisher and producer function\n",
    "\n",
    "Functions are taken from the Week 9 Lab material provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that publishes the message \n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, data)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "\n",
    "        \n",
    "# function that connects the kafka producer        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['192.168.86.48:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "#### Send desired data batches \n",
    "\n",
    "Using the above created functions we create the desired flight data batches and send them so the consumers can receive the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    \n",
    "    ## SET TOPIC AND DATA TO BE SENT\n",
    "    \n",
    "    topic = 'flightTopic'\n",
    "    \n",
    "    all_data = readCSVFile()\n",
    "    \n",
    "    \n",
    "    ## SET THE PRODUCERS\n",
    "    \n",
    "    producer_X = connect_kafka_producer()\n",
    "    producer_Y = connect_kafka_producer()\n",
    "    \n",
    "    \n",
    "    ## GET DATA AND META DATA FOR EACH KEY\n",
    "    \n",
    "    # dict where each key is a DOW and each value\n",
    "    # is a list of flights\n",
    "    key_all_data = prepare_data(all_data)\n",
    "    \n",
    "    # dict where each key is a DOW and each value\n",
    "    # is the length of the list of rows \n",
    "    len_key_all_data = get_len_data(key_all_data)\n",
    "    \n",
    "    # create a dict to count the rows sent so far\n",
    "    data_counter = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}\n",
    "    \n",
    "    # list to save the y data created \n",
    "    storage_y_data = []\n",
    "    \n",
    "    iteration_counter = 0\n",
    "    \n",
    "\n",
    "    # start the data publishing process\n",
    "    print('Publishing records..')\n",
    "    \n",
    "    # set a continous loop to produce and publish data\n",
    "    while True:        \n",
    "        \n",
    "        # loop through the keys from 1 to 7 for each day of the week\n",
    "        for i in range(1,8):\n",
    "            \n",
    "            \n",
    "            ## 1. GET DATA AND META DATA FOR KEY\n",
    "            \n",
    "            # get the data for DOW\n",
    "            key_data = key_all_data[i]\n",
    "            \n",
    "            # get the len of the the data for DOW\n",
    "            len_key_data = len_key_all_data[i]\n",
    "        \n",
    "\n",
    "            ## 2. GET TIMESTAMP FOR EACH RECORD \n",
    "\n",
    "            # creates the current date and time in utc timezone\n",
    "            dt_utc_now = datetime.datetime.utcnow()\n",
    "\n",
    "            # changes the timestamp into the desired format \n",
    "            utc_time_round = dt_utc_now.replace(microsecond=0) \n",
    "\n",
    "            # convert the timestamp to unix-timestamp format\n",
    "            utc_timestamp = utc_time_round.timestamp()\n",
    "            \n",
    "            # save the unix-timestamp as ts\n",
    "            ts = {'ts': int(utc_timestamp)}\n",
    "            \n",
    "            \n",
    "            ## 3. BATCH FOR X\n",
    "            \n",
    "            # generate a random intiger\n",
    "            A = random.randint(70,100)\n",
    "            \n",
    "            # ensure there is enough data to be used \n",
    "            if (data_counter[i] + A) >= len_key_data:\n",
    "                \n",
    "                # if not start from the start again\n",
    "                data_counter[i] = 0\n",
    "                \n",
    "                # select data\n",
    "                x_data = key_data[data_counter[i]: data_counter[i] + A]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # select data\n",
    "                x_data = key_data[data_counter[i]: data_counter[i] + A]\n",
    "            \n",
    "            # increment counter\n",
    "            data_counter[i] += A\n",
    "            \n",
    "            # list that saves the final dicts containing timestamps\n",
    "            final_x_data = []\n",
    "            \n",
    "            # loop through the list of dicts \n",
    "            for x in range(0,len(x_data)):\n",
    "                \n",
    "                # append the timestamp into the object to be sent        \n",
    "                final_x_data.append(dict(x_data[x], **ts))\n",
    "\n",
    "            \n",
    "            ## 4. BATCH FOR Y\n",
    "            \n",
    "            # generate a random intiger\n",
    "            B = random.randint(5,10)\n",
    "            \n",
    "            # ensure there is enough data to be used \n",
    "            if (data_counter[i] + B) >= len_key_data:\n",
    "                \n",
    "                # if not start from the start again\n",
    "                data_counter[i] = 0\n",
    "                \n",
    "                # select data\n",
    "                y_data = key_data[data_counter[i]: data_counter[i] + B]\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # select data\n",
    "                y_data = key_data[data_counter[i]: data_counter[i] + B]\n",
    "            \n",
    "            # increment counter\n",
    "            data_counter[i] += B\n",
    "            \n",
    "            # list that saves the final dicts containing timestamps\n",
    "            final_y_data = []\n",
    "            \n",
    "            # loop through the list of dicts \n",
    "            for x in range(0,len(y_data)):\n",
    "                \n",
    "                # append the timestamp into the object to be sent        \n",
    "                final_y_data.append(dict(y_data[x], **ts))\n",
    "\n",
    "            # append the final_y_data to the list storage_y_data for later use\n",
    "            storage_y_data.append(final_y_data)\n",
    "            \n",
    "            \n",
    "            ##  5. PUBLISH THE MESSAGE\n",
    "            \n",
    "            if iteration_counter == 0:\n",
    "\n",
    "                # for producer x publish the message with the new data\n",
    "                publish_message(producer_X, topic, final_x_data)\n",
    "                \n",
    "                # select the data to send for y\n",
    "                y_data_send = None\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # select the data to send for y\n",
    "                y_data_send = storage_y_data[iteration_counter - 1]\n",
    "                \n",
    "                # for producer x publish the message with the new data\n",
    "                publish_message(producer_X, topic, final_x_data)\n",
    "\n",
    "                # for producer y publish the message with the new data\n",
    "                publish_message(producer_Y, topic, y_data_send)\n",
    "\n",
    "                \n",
    "            ## 6. INCREMENT THE COUNT\n",
    "            \n",
    "            # increment iteration counter\n",
    "            iteration_counter += 1   \n",
    "                \n",
    "                \n",
    "            ## 7. PRINT THE DATA SENT\n",
    " \n",
    "            # print the data being sent\n",
    "            \"\"\"\n",
    "            print('X - DATA')\n",
    "            print(final_x_data)\n",
    "            print('')\n",
    "            print('Y - DATA')\n",
    "            print(y_data_send)\n",
    "            print('-------------------------')\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            # send producer to sleep for 5 seconds\n",
    "            sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
